import json
import logging
import subprocess
from urllib.parse import urlparse
from urllib.robotparser import RobotFileParser
from playwright.sync_api import sync_playwright

# 로깅 설정
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def install_chromium():
    try:
        subprocess.run(["playwright", "install", "chromium"], check=True)
        logger.info("Chromium 설치 완료.")
    except Exception as e:
        logger.error(f"Chromium 설치 실패: {e}")

def check_scrapability(url, user_agent='MyBot'):
    """
    주어진 URL에 대해 로봇 차단 여부 및 HTTP 상태 코드 확인을 수행.
    """
    parsed_url = urlparse(url)
    base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"

    # robots.txt 확인
    rp = RobotFileParser()
    rp.set_url(f"{base_url}/robots.txt")
    rp.read()

    if not rp.can_fetch(user_agent, url):
        return False, "robots.txt에 의해 차단됨"

    # HTTP 상태 코드 확인
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch()
            page = browser.new_page()
            response = page.goto(url)
            status = response.status if response else None

            if status and status >= 400:
                browser.close()
                return False, f"HTTP 상태 코드: {status}"

            page.wait_for_load_state('domcontentloaded')
            content = page.content()
            browser.close()

        if "접근 거부" in content or "Access Denied" in content:
            return False, "접근이 거부됨"

        return True, "스크래핑 가능"
    except Exception as e:
        return False, f"오류 발생: {str(e)}"

def close_overlays(page):
    """
    페이지 오버레이(쿠키 동의, 모달 등) 닫기.
    """
    selectors = [
        '#cookieConsentAgree',
        '.modal-close-button',
    ]
    for selector in selectors:
        try:
            element = page.locator(selector)
            if element.is_visible():
                element.click()
                page.wait_for_load_state('domcontentloaded')
        except Exception as e:
            logger.debug(f"오버레이 닫기 실패 ({selector}): {e}")

def scrape_forbestravelguide(page, keyword):
    """
    Forbes Travel Guide 사이트에서 주어진 keyword로 호텔 정보를 스크래핑.
    """
    info_blocks_dict = {}
    hotel_name = '-'
    content = '-'

    close_overlays(page)

    # 뷰포트 설정
    page.set_viewport_size({"width": 1920, "height": 1080})

    # 검색 수행
    search_selector = '#heroSearch input[name="phrase"][placeholder="Where do you want to go?"]'
    page.locator(search_selector).wait_for(state='visible', timeout=10000)
    page.fill(search_selector, keyword)
    page.press(search_selector, 'Enter')
    page.wait_for_load_state('domcontentloaded')

    # Hotels 탭 클릭
    hotels_tab_selector = '#searchFilters > ul > li.searchFilter.active'
    hotels_tab = page.locator(hotels_tab_selector)
    hotels_tab.wait_for(state='visible', timeout=10000)
    hotels_tab.click()
    page.wait_for_load_state('domcontentloaded')

    # 첫 번째 검색 결과 처리
    result_link_selector = '.searchPage.active h2:has-text("Hotels") + div > a'
    hotel_name_selector = '.searchPage.active h2:has-text("Hotels") + div > div > a'
    result_link = page.locator(result_link_selector)

    if result_link.count() == 0:
        # 결과 없음
        final_results = {
            'keywords': keyword,
            'hotel_name': hotel_name,
            'content': content,
            'info_blocks': info_blocks_dict
        }

        combined_content = f"{content}, {info_blocks_dict}".strip()
        final_dict = {
            "hotel": final_results['hotel_name'],
            "content": combined_content
        }

        # 필요없는 필드 제거 후 result 필드에 최종 결과 저장
        for f in ['keywords', 'hotel_name', 'content', 'info_blocks']:
            del final_results[f]
        final_results['result'] = final_dict

        return final_results

    # 호텔 이름 추출
    hotel_name_element = page.locator(hotel_name_selector)
    if hotel_name_element.is_visible():
        hotel_name = hotel_name_element.inner_text().strip()

    # 첫 번째 결과 페이지 이동
    result_link.first.click()
    page.wait_for_load_state('domcontentloaded')

    # READ MORE 클릭
    read_more_selector = 'a.morelink:has-text("READ MORE")'
    read_more_link = page.locator(read_more_selector)
    if read_more_link.is_visible():
        read_more_link.click()
        page.wait_for_load_state('networkidle')
    else:
        logger.debug("'READ MORE' 링크 없음.")

    # 본문 추출
    content_selector = '#contentLeft > div > div.contentBlock.propDescrption.contentWrap.readMore > span.fullContent'
    content_element = page.locator(content_selector)
    content = content_element.inner_text()

    # 뷰포트 높이 증가
    page.set_viewport_size({"width": 1920, "height": 3000})

    # 정보 블록 추출
    info_block_selector = '#contentLeft > div > div.infoBlockWrap > div'
    info_blocks = page.locator(info_block_selector)

    for i in range(info_blocks.count()):
        info_block = info_blocks.nth(i)
        if not info_block.is_visible():
            continue

        try:
            title_loc = info_block.locator('div.infoBlockTitle.accordion')
            title_loc.wait_for(state='visible', timeout=10000)
            title_loc.evaluate("element => element.scrollIntoView(true)")
            page.wait_for_timeout(500)

            title_text = title_loc.inner_text().strip()
            title_loc.click()
            page.wait_for_timeout(500)

            data_target = title_loc.get_attribute('data-target')
            if not data_target:
                continue

            # li 요소 추출
            li_selector = f"{data_target} li"
            span_selector = f"{data_target} > div > div > span"
            li_elements = page.locator(li_selector)

            if li_elements.count() > 0:
                li_texts = [li_elements.nth(j).inner_text().strip()
                            for j in range(li_elements.count()) if li_elements.nth(j).inner_text().strip()]
            else:
                # li 없으면 span에서 추출
                span_elements = page.locator(span_selector)
                li_texts = [span_elements.nth(j).inner_text().strip()
                            for j in range(span_elements.count()) if span_elements.nth(j).inner_text().strip()]

            info_blocks_dict[title_text] = li_texts

        except Exception as e:
            logger.debug(f"Info Block {i+1} 처리 중 오류: {e}")

    # info_blocks_dict를 문자열로 정리
    transformed_list = []
    for k, v in info_blocks_dict.items():
        # 불필요한 텍스트 제거
        v = [item for item in v if item and item != '{}']
        if k == "Amenities":
            amenities_str = ", ".join(v).replace("{}", "").strip()
            transformed_list.append(f"Amenities are {amenities_str}")
        else:
            transformed_list.append(" ".join(v))

    combined_info_text = " ".join(transformed_list).strip()
    if combined_info_text:
        content = f"{content} {combined_info_text}".strip()

    # \uXXXX 형태, \, 개행 제거
    content = content.replace('\\u', '').replace('\\', '').replace('\n', ' ').strip()

    final_results = {
        'keywords': keyword,
        'hotel_name': hotel_name,
        'content': content,
        'info_blocks': combined_info_text
    }

    combined_content = f"{content}, {combined_info_text}".strip()
    final_dict = {
        "hotel": hotel_name,
        "content": combined_content
    }

    # 불필요한 필드 제거 후 최종 결과 정리
    for f in ['keywords', 'hotel_name', 'content', 'info_blocks']:
        del final_results[f]
    final_results['result'] = final_dict

    return final_results

def gcfIo(request):
    try:
        data = request.get_json()
        keyword = data['calls'][0][0]

        install_chromium()
        url = "https://www.forbestravelguide.com/"
        scrapable, reason = check_scrapability(url)

        if not scrapable:
            return json.dumps({"error": reason}, ensure_ascii=False), 400

        with sync_playwright() as p:
            browser = p.chromium.launch()
            page = browser.new_page()
            page.goto(url)
            final_results = scrape_forbestravelguide(page, keyword)
            browser.close()

        return json.dumps({"replies": [final_results]}, ensure_ascii=False)

    except Exception as e:
        return json.dumps({"error": str(e)}, ensure_ascii=False), 500
